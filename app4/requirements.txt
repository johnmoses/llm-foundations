# Requirements for Multimodal LLM Workflow

# Core dependencies
torch>=1.13.0
torchvision>=0.14.0
torchaudio>=0.13.0

# Transformers and ML libraries
transformers>=4.25.0
accelerate>=0.15.0
tokenizers>=0.13.0

# Image processing
Pillow>=9.0.0
opencv-python>=4.6.0

# Audio processing
librosa>=0.9.0
openai-whisper>=20230314
soundfile>=0.12.0

# Additional utilities
numpy>=1.21.0
scipy>=1.9.0
matplotlib>=3.5.0
tqdm>=4.64.0

# Optional: For better performance
# torch-audio  # For audio processing acceleration
# ffmpeg-python  # For audio format conversion

# Installation instructions:
# 1. Create virtual environment:
#    python -m venv multimodal_env
#    source multimodal_env/bin/activate  # On Windows: multimodal_env\Scripts\activate
#
# 2. Install dependencies:
#    pip install -r requirements.txt
#
# 3. For CUDA support (if you have a compatible GPU):
#    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
#
# 4. Install system dependencies (Ubuntu/Debian):
#    sudo apt-get install ffmpeg libsndfile1-dev
#
# 5. For macOS:
#    brew install ffmpeg
#
# Note: The first run will download several GB of model weights.
# Ensure you have sufficient disk space and internet connection.